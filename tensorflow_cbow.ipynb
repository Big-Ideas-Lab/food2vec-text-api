{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Owner\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import pandas as pd \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "import requests\n",
    "nltk.download('punkt')\n",
    "import gensim\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare query words for querying the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foods = pd.read_csv(\"foods.csv\", delimiter=\",\", skipinitialspace=True)\n",
    "\n",
    "# Get unique words\n",
    "unique_words = foods[\"food_name\"].unique()\n",
    "query_words = list(unique_words) # list of words we will query\n",
    "# query_words = query_words[::2]\n",
    "query_words = random.sample(query_words, 200)\n",
    "\n",
    "def make_query_string(blim, tlim):\n",
    "    string = \"\"\n",
    "    a = 0\n",
    "    for word in query_words[blim:tlim]:\n",
    "        a+=1\n",
    "        word.replace(\",\", \"\")\n",
    "        if a == len(query_words[blim:tlim]):\n",
    "            string = string+word\n",
    "        else:\n",
    "            string = string+word+\",\"\n",
    "    return string\n",
    "\n",
    "def make_all_query_strings():\n",
    "    strings = []\n",
    "    blim = 0\n",
    "    tlim = 100\n",
    "    a = False\n",
    "    while tlim <= len(query_words):\n",
    "        string = make_query_string(blim, tlim)\n",
    "        strings.append(string)\n",
    "        if a is True:\n",
    "            return strings\n",
    "        blim += 100\n",
    "        tlim += 100\n",
    "        if tlim >= len(query_words) and a == False:\n",
    "            tlim = len(query_words)\n",
    "            a = True\n",
    "        print(a)\n",
    "        \n",
    "\n",
    "server_name = \"http://localhost:5000\"\n",
    "\n",
    "def get_corpus():\n",
    "#     strings = make_all_query_strings()\n",
    "    responses = []\n",
    "    a = 0\n",
    "    for string in strings:\n",
    "        r = requests.get(server_name+\"/recipe?query={}&number=10\".format(string))\n",
    "        if r.status_code != 200:\n",
    "            print(\"Error: {} - {}\".format(r.status_code, r.text))\n",
    "        else:\n",
    "            print(\"Success\")\n",
    "        responses.append(r.json())\n",
    "        print(a)\n",
    "        a+=1\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and Save Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = get_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON so that we don't have to query again\n",
    "my_json = json.dumps(corpus)\n",
    "with open(\"corpus0-150.json\", \"w\") as f:\n",
    "    json.dump(my_json, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call load corpus.json if already ran get_corpus(), make vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"corpus0-50_3.json\", \"r\") as f:\n",
    "    corpus = json.load(f)\n",
    "corpus=json.loads(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for response in corpus: # for query word response\n",
    "    for query in response:\n",
    "        if query[\"corpus\"] == \"\":\n",
    "            continue\n",
    "        else:\n",
    "            paragraph = query[\"corpus\"].lower()\n",
    "            sentences = re.split(r\"[.!?]\", paragraph)\n",
    "            for sentence in sentences: \n",
    "                sentence = sentence.replace(\",\", \"\")\n",
    "                nltk_tokens = word_tokenize(sentence)\n",
    "                nltk_tokens = [word for word in nltk_tokens if len(word)>1 and word.isalpha()]\n",
    "                words.extend(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(words)\n",
    "char_to_int = dict((c,i) for i,c in enumerate(vocab))\n",
    "int_to_char = dict((i,c) for i,c in enumerate(vocab))\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "temp_dict = []\n",
    "window_size = 10\n",
    "for i in range(len(words)):\n",
    "    a = i-window_size\n",
    "    b= i+window_size\n",
    "    curr_word = words[i]\n",
    "    for z in range(a,i):\n",
    "        if z >=0:\n",
    "            temp_dict.append((curr_word,words[z]))\n",
    "    for z in range(i+1,b):\n",
    "        if z<len(vocab):\n",
    "            temp_dict.append((curr_word,words[z]))\n",
    "for pair in temp_dict:\n",
    "    tempx = np.zeros(len(vocab))\n",
    "    tempy = np.zeros(len(vocab))\n",
    "    tempx[char_to_int[pair[0]]] = 1\n",
    "    tempy[char_to_int[pair[1]]] = 1\n",
    "    X.append(tempx)\n",
    "    Y.append(tempy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings and Weight Updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "batch_size = 64\n",
    "epochs = 100 \n",
    "n_batches = int(len(X)/batch_size)\n",
    "learning_rate= 0.001\n",
    "x = tf.placeholder(tf.float32,shape = (None,len(vocab)))\n",
    "y = tf.placeholder(tf.float32,shape = (None,len(vocab)))\n",
    "w1 = tf.Variable(tf.random_normal([len(vocab),embedding_size]),dtype = tf.float32)\n",
    "b1 = tf.Variable(tf.random_normal([embedding_size]),dtype = tf.float32)\n",
    "w2 = tf.Variable(tf.random_normal([embedding_size,len(vocab)]),dtype = tf.float32)\n",
    "b2 = tf.Variable(tf.random_normal([len(vocab)]),dtype = tf.float32)\n",
    "hidden_y = tf.matmul(x,w1) + b1\n",
    "y_pred = tf.matmul(hidden_y,w2) + b2\n",
    "cost = tf.reduce_mean(tf.losses.mean_squared_error(y_pred,y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.33)\n",
    "sess = tf.Session(config = tf.ConfigProto(gpu_options = gpu_options))\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(5):\n",
    "    avg_cost = 0\n",
    "    for i in range(n_batches-1):\n",
    "        batch_x = X[i*batch_size:(i+1)*batch_size]\n",
    "        batch_y = Y[i*batch_size:(i+1)*batch_size]\n",
    "        #print(batch_x.shape)\n",
    "        _,c = sess.run([optimizer,cost],feed_dict = {x:batch_x,y:batch_y})\n",
    "        #print(test.shape)\n",
    "        \n",
    "        avg_cost += c/n_batches\n",
    "    print('Epoch',epoch,' - ',avg_cost)\n",
    "save_path = saver.save(sess,'/Users/Owner/repos/food2vec-text-api/weights.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the words embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = dict()\n",
    "for i in vocab:\n",
    "    temp_a = np.zeros([1,len(vocab)])\n",
    "    temp_a[0][char_to_int[i]] = 1\n",
    "    temp_emb = sess.run([y_pred],feed_dict = {x:temp_a})\n",
    "    temp_emb = np.array(temp_emb)\n",
    "    embeddings[i] = temp_emb.reshape([len(vocab)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the closest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(word,n):\n",
    "    distances = dict()\n",
    "    for w in embeddings.keys():\n",
    "        distances[w] = cosine_similarity(embeddings[w],embeddings[word])\n",
    "    d_sorted = OrderedDict(sorted(distances.items(),key = lambda x:x[1] ,reverse = True))\n",
    "    s_words = d_sorted.keys()\n",
    "    print(s_words[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "tokens = []\n",
    "for w in embeddings.keys():\n",
    "    labels.append(w)\n",
    "    tokens.append(embeddings[w])\n",
    "tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "new_values = tsne_model.fit_transform(tokens)\n",
    "x = []\n",
    "y = []\n",
    "for value in new_values:\n",
    "    x.append(value[0])\n",
    "    y.append(value[1])\n",
    "    \n",
    "plt.figure(figsize=(16, 16)) \n",
    "for i in range(len(x)):\n",
    "    plt.scatter(x[i],y[i])\n",
    "    plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}